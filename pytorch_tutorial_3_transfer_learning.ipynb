{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn.functional as F\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torchvision\n",
    "from torchvision import datasets, models, transforms\n",
    "from torch.utils.data import Dataset\n",
    "import time\n",
    "import os\n",
    "\n",
    "import random\n",
    "from torchvision.io import read_image\n",
    "from PIL import Image\n",
    "from sklearn.metrics import confusion_matrix, accuracy_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "when we train a neural network we need:\n",
    "\n",
    "- \n",
    "-\n",
    "-\n",
    "-\n",
    "- \n",
    "- \n",
    "\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "In this tutorial we will do transfer learning.\n",
    "\n",
    "Generally, this means taking a model that was trained on one dataset, called a \"pre-trained model\", \n",
    "and use it in \"some way\" to be applied towards performing classification (inference in general) on \n",
    "some other dataset, sampled from a different data distribution, and usually always with a completely\n",
    "different set of classes.\n",
    "\n",
    "What are \"some ways\" we can use a pre-trained model?\n",
    "\n",
    "1. fine-tune \n",
    "- view it as a more favorable \"random initialization\"\n",
    "- do not freeze the pre-trained model\n",
    "- what do I mean by freeze?\n",
    "\n",
    "\n",
    "2. embedder \n",
    "- we must freeze the pre-trained model\n",
    "-map inputs from new dataset to features that are outputs of pre-trained model\n",
    "- these embeddings should carry semantic information on the classes on the new dataset, though not perfectly\n",
    "- can build a simple classifier on top of this embedding layer\n",
    "- we can look at the embedder + new simple classifier as either a new model, or we can take the perspective\n",
    "that we are simply using the pre-trained model to map data instances to new features, and the new features\n",
    "are the inputs for a new smaller model\n",
    "\n",
    "\n",
    "Some questions we might need to ask ourselves?\n",
    "\n",
    "\"Which layer do we use for the output layer of the pre-trained model?\"\n",
    "\n",
    "Framed differently,\n",
    "\n",
    "\"Where do we cut off the head of the network?\"\n",
    "\n",
    "Both of which should lead us to ask:\n",
    "\n",
    "\"How do we determine which layer to use for the output of the pre-trained model?\"\n",
    "\n",
    "\n",
    "\n",
    "For this tutorial, we will use a model pre-trained on IMAGENET_1K (1000 classes), and will\n",
    "apply this to the downstream dataset: \n",
    "\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# train epoch\n",
    "def train_epoch(model, optimizer, loss_func, dataloader):\n",
    "    # for metrics computations\n",
    "    epoch_preds = []\n",
    "    epoch_labels = []\n",
    "    \n",
    "    # set the model to be in \"train\" model\n",
    "    model.train()\n",
    "    \n",
    "    # iterate through the dataloader, batch by batch\n",
    "    for i, batch in enumerate(dataloader):\n",
    "        \n",
    "        images, labels = batch\n",
    "        \n",
    "        # pass to GPU?\n",
    "        \n",
    "        # zero the gradients\n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        # make sure we are tracking gradients from here on out\n",
    "        with torch.set_grad_enabled(True):\n",
    "            \n",
    "            # pass data through the network\n",
    "            output = model(images)\n",
    "            \n",
    "            # compute the loss\n",
    "            loss = loss_func(output, labels)\n",
    "                        \n",
    "            # call back-prop\n",
    "            loss.backward()\n",
    "            \n",
    "            # do a step of gradient descent\n",
    "            optimizer.step()\n",
    "        \n",
    "        \n",
    "        # now let's update our metrics\n",
    "        with torch.no_grad():\n",
    "            _, preds = torch.max(output, 1)\n",
    "            preds = preds.detach().tolist()\n",
    "            epoch_preds.extend(preds)\n",
    "            epoch_labels.extend(labels.detach().tolist())\n",
    "    \n",
    "    return epoch_labels, epoch_preds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# validation epoch\n",
    "def val_epoch(model, dataloader):\n",
    "    # for metrics computations\n",
    "    epoch_preds = []\n",
    "    epoch_labels = []\n",
    "    \n",
    "    # set the model to be in \"train\" model\n",
    "    model.eval()\n",
    "    \n",
    "    # iterate through the dataloader, batch by batch\n",
    "    for i, batch in enumerate(dataloader):\n",
    "        \n",
    "        images, labels = batch\n",
    "        \n",
    "        # pass to GPU?\n",
    "     \n",
    "        # don't track gradients\n",
    "        with torch.no_grad():\n",
    "            \n",
    "            # pass data through the network\n",
    "            output = model(images)\n",
    "            \n",
    "            # metrics\n",
    "            _, preds = torch.max(output, 1)\n",
    "            preds = preds.detach().tolist()\n",
    "            epoch_preds.extend(preds)\n",
    "            epoch_labels.extend(labels.detach().tolist())\n",
    "    \n",
    "    return epoch_labels, epoch_preds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# hyper-parameters\n",
    "BATCH_SIZE = 32\n",
    "LEARNING_RATE = 0.1\n",
    "NUM_EPOCHS = 10\n",
    "NUM_CLASSES = 101\n",
    "TRAIN_SPLIT = 0.75\n",
    "EMBEDDING_DIM = 512\n",
    "MODEL_WEIGHT_SAVE_PATH = '/home/jovyan/multimodal-vol-1/MLTS/best-transferlearning-weights.pth'\n",
    "PATH = '/home/jovyan/multimodal-vol-1/MLTS/CALTECH101/caltech-101/101_ObjectCategories/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# transforms\n",
    "normalize = transforms.Normalize(mean=[0.485, 0.456, 0.406],\n",
    "                                 std=[0.229, 0.224, 0.225])\n",
    "\n",
    "preprocessing = transforms.Compose([\n",
    "    transforms.Scale(256),\n",
    "    transforms.CenterCrop(224),\n",
    "    transforms.ToTensor(),\n",
    "    normalize,\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def sample_image_for_viewing():\n",
    "    class_folders = os.listdir(PATH)\n",
    "    # remove \"Faces_easy\" class, and use only the \"Faces\" class\n",
    "    class_folders = [c for c in class_folders if c != 'Faces_easy'] \n",
    "    \n",
    "    class_ = np.random.choice(class_folders)\n",
    "    print(\"Class: \", class_)\n",
    "    \n",
    "    class_path = os.path.join(PATH, class_)\n",
    "    files = os.listdir(class_path)\n",
    "    instance = np.random.choice(files)\n",
    "    \n",
    "    img = Image.open(os.path.join(class_path, instance))\n",
    "    return img\n",
    "   \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "img = sample_image_for_viewing()\n",
    "img"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "y = preprocessing(img)\n",
    "y.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Dataset objects\n",
    "\n",
    "class CALTECH101_DATASET(Dataset):\n",
    "    \"\"\"\n",
    "    A dataset where we *need* to load data on the fly\n",
    "    \n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, input_paths, labels, transform=None):\n",
    "        \"\"\"\n",
    "        args:\n",
    "        \n",
    "        input_paths (dict): mapping dataset indices to path of image\n",
    "        labels (torch.tensor): labels \n",
    "        transform (torchvision.transform): transforms for dataset\n",
    "        \"\"\"\n",
    "        \n",
    "        self.input_paths = input_paths\n",
    "        self.inputs = dict()\n",
    "        self.labels = labels\n",
    "        self.transform = transform\n",
    "        self.N = len(self.labels)\n",
    "\n",
    "    def __len__(self):\n",
    "        return self.N\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        img_path = self.input_paths[idx]\n",
    "        image = Image.open(img_path)\n",
    "        if self.transform:\n",
    "            image = self.transform(image)\n",
    "        return image, self.labels[idx]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def is_single_channel_images(img):\n",
    "    preprocessing = transforms.Compose([\n",
    "                                        transforms.ToTensor()\n",
    "                                        ])\n",
    "    img = preprocessing(img)\n",
    "    if img.shape[0] != 3:\n",
    "        return True\n",
    "    \n",
    "    return False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Build Caltech101 dataset objects\n",
    "def build_caltech101_datasets(path, train_split=0.8, transforms=None):\n",
    "    \"\"\"\n",
    "    Caltech is composed of 101 classes, total of 8711 data points.\n",
    "    Builds a train and validation dataset according to the train_split.\n",
    "    \n",
    "    args:\n",
    "    -----\n",
    "    \n",
    "    path (str): path/to/class/folders/\n",
    "    train_split (float): proportion of dataset that belongs to training dataset.\n",
    "    \n",
    "    returns:\n",
    "    --------\n",
    "    \n",
    "    train dataset and val dataset objects.\n",
    "    \"\"\"\n",
    "    \n",
    "    train_counter = 0\n",
    "    val_counter = 0\n",
    "    \n",
    "    train_path_dict = dict()\n",
    "    val_path_dict = dict()\n",
    "    \n",
    "    train_labels = []\n",
    "    val_labels = []\n",
    "    \n",
    "    class_to_idx_mapper_dict = dict()\n",
    "    \n",
    "    class_folders = os.listdir(path)\n",
    "    # remove \"Faces_easy\" class, and use only the \"Faces\" class\n",
    "    class_folders = [c for c in class_folders if c != 'Faces_easy'] \n",
    "    \n",
    "    # map class names (strings) to class indices\n",
    "    for i in range(len(class_folders)):\n",
    "        class_to_idx_mapper_dict[class_folders[i]] = i\n",
    "        \n",
    "    # let's split up the data class by class\n",
    "    for class_folder in class_folders:\n",
    "        class_idx = class_to_idx_mapper_dict[class_folder]\n",
    "        \n",
    "        # get number of instanes for this class\n",
    "        this_class_path = os.path.join(path, class_folder)\n",
    "        files = os.listdir(this_class_path)\n",
    "        files = [f for f in files if '.ipy' not in f]\n",
    "        files = [f for f in files if is_single_channel_images(Image.open(os.path.join(this_class_path,f))) is False]\n",
    "        num_instances = len(files)\n",
    "        \n",
    "        # compute number in train/val splits\n",
    "        num_train = int(num_instances*train_split)\n",
    "        num_val = num_instances - num_train\n",
    "        \n",
    "        # random sample\n",
    "        random.shuffle(files)\n",
    "        train_instances = files[:num_train]\n",
    "        val_instances = files[num_train:]\n",
    "        \n",
    "        # add train instances\n",
    "        for inst in train_instances:\n",
    "            inst_path = os.path.join(this_class_path, inst)\n",
    "            train_path_dict[train_counter] = inst_path\n",
    "            train_counter += 1\n",
    "            \n",
    "        # add train labels\n",
    "        train_labels.extend([class_idx]*num_train)\n",
    "        \n",
    "        for inst in val_instances:\n",
    "            inst_path = os.path.join(this_class_path, inst)\n",
    "            val_path_dict[val_counter] = inst_path\n",
    "            val_counter += 1\n",
    "            \n",
    "        # add val labels\n",
    "        val_labels.extend([class_idx]*num_val)\n",
    "    \n",
    "    # build the datasets\n",
    "    train = CALTECH101_DATASET(train_path_dict, \n",
    "                               torch.tensor(train_labels), \n",
    "                               transforms)\n",
    "    val = CALTECH101_DATASET(val_path_dict, \n",
    "                             torch.tensor(val_labels), \n",
    "                             transforms)\n",
    "    \n",
    "    return train, val\n",
    "    \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# build the datasets\n",
    "train, val = build_caltech101_datasets(PATH, TRAIN_SPLIT, preprocessing)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# dataloaders\n",
    "train_dataloader = torch.utils.data.DataLoader(train,\n",
    "                                              batch_size = BATCH_SIZE,\n",
    "                                              shuffle = True\n",
    "                                              )\n",
    "\n",
    "val_dataloader = torch.utils.data.DataLoader(val,\n",
    "                                              batch_size = BATCH_SIZE,\n",
    "                                              shuffle = False\n",
    "                                              )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# pick our pre-trained model architecture\n",
    "from torchvision.models import resnet50, resnet18\n",
    "\n",
    "# Using pretrained weights:\n",
    "net = resnet18(pretrained=True)\n",
    "print(net)\n",
    "\n",
    "params = filter(lambda p: p.requires_grad, net.parameters())\n",
    "num_params = sum([np.prod(p.size()) for p in params])\n",
    "print(\"Model parameters: \", num_params)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# freeze the weights\n",
    "for p in net.parameters():\n",
    "    p.requires_grad = False\n",
    "\n",
    "params = filter(lambda p: p.requires_grad, net.parameters())\n",
    "num_params = sum([np.prod(p.size()) for p in params])\n",
    "print(\"Model parameters: \", num_params)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "either change net.fc to something else (single layer, or a  Sequential)\n",
    "OR modify source code?\n",
    "\n",
    "modules = list(net.children())[:-1]\n",
    "resnet = nn.Sequential(*modules)\n",
    "\n",
    "\n",
    "\n",
    "OR\n",
    "\n",
    "\n",
    "class Identity(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Identity, self).__init__()\n",
    "        \n",
    "    def forward(self, x):\n",
    "        return x\n",
    "        \n",
    "net.fc = Identity()\n",
    "\n",
    "\n",
    "\n",
    "OR\n",
    "\n",
    "\n",
    "net.fc = nn.Linear(EMBEDDING_DIM, NUM_CLASSES)\n",
    "\n",
    "\n",
    "\n",
    "OR\n",
    "\n",
    "\n",
    "net.fc = nn.Sequential(put,\n",
    "                        my,\n",
    "                        layers,\n",
    "                        and,\n",
    "                        activations,\n",
    "                        here\n",
    "                        )\n",
    "\n",
    "\"\"\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "net.fc = nn.Linear(EMBEDDING_DIM, NUM_CLASSES)\n",
    "\n",
    "params = filter(lambda p: p.requires_grad, net.parameters())\n",
    "num_params = sum([np.prod(p.size()) for p in params])\n",
    "print(\"Model parameters: \", num_params)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "print(net)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# optimizer\n",
    "optimizer = torch.optim.AdamW(net.parameters(), \n",
    "                              lr=LEARNING_RATE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# loss\n",
    "criteria = nn.CrossEntropyLoss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# tracking metrics (let's use scikitlearn now)\n",
    "train_accs = []\n",
    "val_accs = []\n",
    "best_acc = -1.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# training loop\n",
    "for epoch in range(1, NUM_EPOCHS+1):\n",
    "    print(\"Epoch: \", epoch, \" of \", NUM_EPOCHS+1)\n",
    "    \n",
    "    # train\n",
    "    labels, preds = train_epoch(net, optimizer, criteria, train_dataloader)\n",
    "    \n",
    "    # metrics\n",
    "    cm, acc = confusion_matrix(labels, preds), accuracy_score(labels, preds)\n",
    "    print(\"Train accuracy: \", acc)\n",
    "    train_accs.append(acc)\n",
    "    \n",
    "    \n",
    "    # validation\n",
    "    labels, preds = val_epoch(net, val_dataloader)\n",
    "    \n",
    "    # metrics\n",
    "    cm, acc = confusion_matrix(labels, preds), accuracy_score(labels, preds)\n",
    "    print(\"Validation accuracy: \", acc)\n",
    "    val_accs.append(acc)\n",
    "    \n",
    "    \n",
    "    # update best model\n",
    "    if acc > best_acc:\n",
    "        best_acc = acc\n",
    "        print(\"We've got ourselves a winner over here!\")\n",
    "        print(cm)\n",
    "        state_dict = {'weights': net.state_dict(),\n",
    "                     'epoch': epoch,\n",
    "                      'val_acc': acc.item()\n",
    "                     }\n",
    "        torch.save(state_dict, MODEL_WEIGHT_SAVE_PATH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "results = {'train': train_accs, 'val': val_accs}\n",
    "torch.save(results,'/home/jovyan/multimodal-vol-1/MLTS/resnet18_pretrained_frozen_results.pth')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# plotting\n",
    "plt.plot(train_accs, color='r', label=\"Train\")\n",
    "plt.plot(val_accs, color='k', label=\"Validation\")\n",
    "plt.xlabel(\"Epoch\")\n",
    "plt.ylabel(\"Accuracy\")\n",
    "plt.title()\n",
    "plt.legend()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ok, what about if we trained from scratch? what if we fine-tune?\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
