{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn.functional as F\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torchvision\n",
    "from torchvision import datasets, models, transforms\n",
    "import time\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Today we will train a neural network to classify images of handwritten digits using the MNIST dataset.\n",
    "MNIST dataset can be loaded directly from PyTorch, as can many datasets.\n",
    "\n",
    "https://pytorch.org/vision/stable/datasets.html\n",
    "\n",
    "\n",
    "What do we need to train a model?\n",
    "- \n",
    "-\n",
    "-\n",
    "-\n",
    "-\n",
    "-\n",
    "-\n",
    "\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# transforms\n",
    "mnist_transforms = transforms.Compose([\n",
    "                                        transforms.ToTensor(),\n",
    "                                        transforms.Normalize([0.5], [0.5])\n",
    "                                        ])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# load train/validation MNIST datasets\n",
    "mnist_train_data = datasets.MNIST('/home/jovyan/MNIST/', train=True, download=True, transform=mnist_transforms)\n",
    "mnist_val_data = datasets.MNIST('/home/jovyan/MNIST/', train=False, download=True, transform=mnist_transforms)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# define some hypers\n",
    "BATCH_SIZE = 32\n",
    "LEARNING_RATE = 0.01\n",
    "NUM_EPOCHS = 5\n",
    "MODEL_WEIGHT_SAVE_PATH = '/home/jovyan/best-mnist-val-weights.pth'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# data loader\n",
    "train_dataloader = torch.utils.data.DataLoader(mnist_train_data,\n",
    "                                              batch_size = BATCH_SIZE,\n",
    "                                              shuffle = True\n",
    "                                              )\n",
    "\n",
    "val_dataloader = torch.utils.data.DataLoader(mnist_val_data,\n",
    "                                              batch_size = BATCH_SIZE,\n",
    "                                              shuffle = False\n",
    "                                              )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# define a neural network\n",
    "class The_best_neural_network(nn.Module):\n",
    "    def __init__(self, embedding_dim = 128):\n",
    "        super(The_best_neural_network, self).__init__()\n",
    "        \n",
    "        # NOTE: input shape is (1, 28, 28) but we want to map it to 784 (for simple MLP)\n",
    "        self.embedding_dim = embedding_dim\n",
    "        \n",
    "        # TODO: implement this\n",
    "\n",
    "        \n",
    "    def forward(self, x):\n",
    "\n",
    "        \n",
    "        # TODO\n",
    "        return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# define a training epoch\n",
    "def do_training_epoch(model, dataloader, loss_func, optimizer):\n",
    "    # init some metrics\n",
    "    num_instances = 0\n",
    "    num_correct = 0\n",
    "    running_loss = 0.0\n",
    "    \n",
    "    \n",
    "    # set the model to be in \"train\" model\n",
    "    model.train()\n",
    "    \n",
    "    # iterate through the dataloader, batch by batch\n",
    "    \n",
    "    for i, batch in enumerate(dataloader):\n",
    "        \n",
    "        x, labels = batch\n",
    "        \n",
    "        # pass to GPU?\n",
    "        \n",
    "        \n",
    "        # zero the gradients\n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        # make sure we are tracking gradients from here on out\n",
    "        with torch.set_grad_enabled(True):\n",
    "            # pass data through the network\n",
    "            output = model(x)\n",
    "            \n",
    "            # compute the loss\n",
    "            loss = loss_func(output, labels)\n",
    "            \n",
    "            # call back-prop\n",
    "            loss.backward()\n",
    "            \n",
    "            # do a step of gradient descent\n",
    "            optimizer.step()\n",
    "        \n",
    "        \n",
    "        # now let's update our metrics\n",
    "        with torch.no_grad():\n",
    "            _, preds = torch.max(output, 1)\n",
    "            running_loss += loss.item() * x.size(0)\n",
    "            num_correct += torch.sum(preds == labels.data)\n",
    "            num_instances += x.size(0)\n",
    "    mean_loss = running_loss / num_instances\n",
    "    mean_accuracy = num_correct / num_instances\n",
    "    \n",
    "    return mean_accuracy, mean_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# define a validation epoch\n",
    "def do_validation_epoch(model, dataloader, loss_func):\n",
    "    # init some metrics\n",
    "    num_instances = 0\n",
    "    num_correct = 0\n",
    "    running_loss = 0.0\n",
    "    \n",
    "    # set the model to be in \"evaluation\" model\n",
    "    model.eval()\n",
    "    \n",
    "    # iterate through the dataloader, batch by batch\n",
    "    \n",
    "    for i, batch in enumerate(dataloader):\n",
    "        \n",
    "        x, labels = batch\n",
    "        \n",
    "        # pass to GPU?\n",
    "        \n",
    "\n",
    "        # make sure we are tracking gradients from here on out\n",
    "        with torch.no_grad():\n",
    "            # pass data through the network\n",
    "            output = model(x)\n",
    "            \n",
    "            # compute the loss\n",
    "            loss = loss_func(output, labels)\n",
    "\n",
    "            # now let's update our metrics\n",
    "            _, preds = torch.max(output, 1)\n",
    "            running_loss += loss.item() * x.size(0)\n",
    "            num_correct += torch.sum(preds == labels.data)\n",
    "            num_instances += x.size(0)\n",
    "    mean_loss = running_loss / num_instances\n",
    "    mean_accuracy = num_correct / num_instances\n",
    "    \n",
    "    return mean_accuracy, mean_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# build model\n",
    "net = The_best_neural_network()\n",
    "\n",
    "print(\"Model structure: \", net)\n",
    "\n",
    "params = filter(lambda p: p.requires_grad, net.parameters())\n",
    "num_params = sum([np.prod(p.size()) for p in params])\n",
    "print(\"Model parameters: \", num_params)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# build optimizer\n",
    "optimizer = optim.SGD(net.parameters(), \n",
    "                      lr=LEARNING_RATE, \n",
    "                      momentum=0.9)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# build loss function\n",
    "loss_func = nn.CrossEntropyLoss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# keeping track of metrics\n",
    "\n",
    "best_val_acc = 0.0\n",
    "training_losses = []\n",
    "training_accs = []\n",
    "val_losses = []\n",
    "val_accs = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# train model, \n",
    "for epoch in range(NUM_EPOCHS):\n",
    "    print(\"Let's do it up, epoch number: \", (epoch+1), \" of: \", NUM_EPOCHS)\n",
    "    \n",
    "    # train epoch\n",
    "    epoch_acc, epoch_loss = do_training_epoch(net, train_dataloader, loss_func, optimizer)\n",
    "    \n",
    "    print(\"Training loss: \", epoch_loss, \" and accuracy: \", epoch_acc.item())\n",
    "    \n",
    "    # update metrics\n",
    "    training_losses.append(epoch_loss)\n",
    "    training_accs.append(epoch_acc.item())\n",
    "    \n",
    "    # val epoch\n",
    "    epoch_acc, epoch_loss = do_validation_epoch(net, val_dataloader, loss_func)\n",
    "    \n",
    "    print(\"Validation loss: \", epoch_loss, \" and accuracy: \", epoch_acc.item())\n",
    "    \n",
    "    # update metrics\n",
    "    val_losses.append(epoch_loss)\n",
    "    val_accs.append(epoch_acc.item())\n",
    "    \n",
    "    # is this the best epoch yet? if so, let's save the model\n",
    "    if epoch_acc > best_val_acc:\n",
    "        best_val_acc = epoch_acc\n",
    "        state_dict = {'weights': net.state_dict(),\n",
    "                     'epoch': epoch,\n",
    "                      'val_acc': epoch_acc.item()\n",
    "                     }\n",
    "        torch.save(state_dict, MODEL_WEIGHT_SAVE_PATH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# evaluation plots\n",
    "\n",
    "plt.plot(training_accs, color='r', label='Training')\n",
    "plt.plot(val_accs, color='k', label=\"Validation\")\n",
    "plt.title(\"Accuracy plots\")\n",
    "plt.ylabel(\"Acc.\")\n",
    "plt.xlabel(\"Epoch\")\n",
    "plt.legend()\n",
    "plt.show()\n",
    "plt.close()\n",
    "\n",
    "plt.plot(training_losses, color='r', label='Training')\n",
    "plt.plot(val_losses, color='k', label=\"Validation\")\n",
    "plt.title(\"Loss curves\")\n",
    "plt.ylabel(\"CE Loss\")\n",
    "plt.xlabel(\"Epoch\")\n",
    "plt.legend()\n",
    "plt.show()\n",
    "plt.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# load best model\n",
    "my_model = The_best_neural_network()\n",
    "loaded_state_dict = torch.load(MODEL_WEIGHT_SAVE_PATH)\n",
    "my_model.load_state_dict(loaded_state_dict['weights'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# let's look at a batch ourselves\n",
    "print(my_model.fc.weight.shape)\n",
    "my_model.fc.bias\n",
    "\n",
    "batch = next(iter(val_dataloader))\n",
    "x, labels = batch\n",
    "print(x.shape)\n",
    "x = x.view(-1, 784)\n",
    "print(x.shape)\n",
    "\n",
    "out = my_model(x)\n",
    "print(out.shape)\n",
    "print(out[:4])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# these are the logits, what about softmax \"probability\" scores\n",
    "softmax = F.softmax(out)\n",
    "print(softmax[:4])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "_, preds = torch.max(softmax, 1)\n",
    "print(preds)\n",
    "print(labels)\n",
    "print((preds == labels).sum().item())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# pass data through layer by layer"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
