{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# mnist addition using conv nets!!! multiple inputs (2 streams), \n",
    "# and multiple outputs (losses to predict each of the digits, plus addition)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn.functional as F\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torchvision\n",
    "from torchvision import datasets, models, transforms\n",
    "from torch.utils.data import Dataset\n",
    "import time\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# transforms\n",
    "mnist_transforms = transforms.Compose([\n",
    "                                        transforms.ToTensor(),\n",
    "                                        transforms.Normalize([0.5], [0.5])\n",
    "                                        ])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# load train/validation MNIST datasets\n",
    "mnist_train_data = datasets.MNIST('/home/jovyan/MNIST/', train=True, download=False, transform=mnist_transforms)\n",
    "mnist_val_data = datasets.MNIST('/home/jovyan/MNIST/', train=False, download=False, transform=mnist_transforms)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "print(len(mnist_train_data))\n",
    "print(type(mnist_train_data[0]))\n",
    "img, label = mnist_train_data[0]\n",
    "print(label)\n",
    "plt.imshow(img.reshape(28,28))\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "class Mnist_Custom_Dataset(Dataset):\n",
    "    \"\"\"\n",
    "    Custom MNIST dataset that returns 2 digits (images), their respective labels\n",
    "    and the summation of the two numbers (labels).\n",
    "    \"\"\"\n",
    "    def __init__(self, images, labels):\n",
    "        \"\"\"\n",
    "        args:\n",
    "        \n",
    "        images (torch.tensor): tensor of all the inputs (images) for a dataset\n",
    "        labels (torch.tensor): tensor of the labels corresponding to inputs\n",
    "        \"\"\"\n",
    "        \n",
    "        self.images = images\n",
    "        self.labels = labels\n",
    "        self.N = len(self.labels)\n",
    "\n",
    "    def __len__(self):\n",
    "        return self.N\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        # dataset will return 2 digits and their labels, as well as their summation as the \"true\" label\n",
    "        first_image = self.images[idx]\n",
    "        first_label = self.labels[idx]\n",
    "        \n",
    "        # sample a random image for the 2nd digit\n",
    "        second_idx = torch.randint(0, self.N, (1,)).item()\n",
    "        second_image = self.images[second_idx]\n",
    "        second_label = self.labels[second_idx]\n",
    "        \n",
    "        summation_label = first_label + second_label\n",
    "        \n",
    "        return first_image, first_label, second_image, second_label, summation_label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# separate the inputs from labels\n",
    "mnist_train_inputs = [mnist_train_data[i][0] for i in range(len(mnist_train_data))]\n",
    "mnist_train_labels = [torch.tensor(mnist_train_data[i][1]) for i in range(len(mnist_train_data))]\n",
    "\n",
    "mnist_val_inputs = [mnist_val_data[i][0] for i in range(len(mnist_val_data))]\n",
    "mnist_val_labels = [torch.tensor(mnist_val_data[i][1]) for i in range(len(mnist_val_data))]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "mnist_train_inputs = torch.stack(mnist_train_inputs)\n",
    "print(mnist_train_inputs.shape)\n",
    "mnist_train_labels = torch.stack(mnist_train_labels)\n",
    "print(mnist_train_labels.shape)\n",
    "\n",
    "mnist_val_inputs = torch.stack(mnist_val_inputs)\n",
    "print(mnist_val_inputs.shape)\n",
    "mnist_val_labels = torch.stack(mnist_val_labels)\n",
    "print(mnist_val_labels.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# now build our custom dataset\n",
    "custom_mnist_train_dataset = Mnist_Custom_Dataset(mnist_train_inputs, mnist_train_labels)\n",
    "custom_mnist_val_dataset = Mnist_Custom_Dataset(mnist_val_inputs, mnist_val_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# just get data from the dataset\n",
    "digit_1, label_1, digit_2, label_2, summation_label = custom_mnist_train_dataset.__getitem__(0)\n",
    "print(label_1, label_2, summation_label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define some hypers\n",
    "BATCH_SIZE = 32\n",
    "LEARNING_RATE = 0.01\n",
    "NUM_EPOCHS = 5\n",
    "MODEL_WEIGHT_SAVE_PATH = '/home/jovyan/best-mnist-summation-val-weights.pth'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# data loader\n",
    "train_dataloader = torch.utils.data.DataLoader(custom_mnist_train_dataset,\n",
    "                                              batch_size = BATCH_SIZE,\n",
    "                                              shuffle = True\n",
    "                                              )\n",
    "\n",
    "val_dataloader = torch.utils.data.DataLoader(custom_mnist_val_dataset,\n",
    "                                              batch_size = BATCH_SIZE,\n",
    "                                              shuffle = False\n",
    "                                              )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# define a neural network\n",
    "class SummationNet(nn.Module):\n",
    "    \"\"\"\n",
    "    A CNN that has 2 input channels, and 3 output heads.\n",
    "    This CNN takes 2 MNIST images and inputs and aims to \n",
    "    do summation of the 2 digits (integers).\n",
    "    \n",
    "    \"\"\"\n",
    "    def __init__(self):\n",
    "        super(SummationNet, self).__init__()\n",
    "        \n",
    "        # NOTE: input shape is (1, 28, 28) \n",
    "        \n",
    "        # should we use 2 separate input branches or use \"parameter\" sharing\n",
    "        # via a single input branch?\n",
    "        self.conv1 = nn.Conv2d(1, 16, 3, stride=2)\n",
    "        self.conv2 = nn.Conv2d(16, 16, 3)\n",
    "        self.conv3 = nn.Conv2d(16, 32, 3)\n",
    "        self.pooling = nn.AvgPool2d(kernel_size = 3, stride=3)\n",
    "        # representation will now be of shape (batch_size, 32, 3, 3)\n",
    "\n",
    "        # sequential\n",
    "        self.input_branch_2 = nn.Sequential(\n",
    "                                          nn.Conv2d(1, 16, 3, stride=2),\n",
    "                                          nn.ReLU(),\n",
    "                                          nn.Conv2d(16, 16, 3),\n",
    "                                          nn.ReLU(),\n",
    "                                          nn.Conv2d(16, 32, 3),\n",
    "                                          nn.ReLU(),\n",
    "                                          nn.AvgPool2d(kernel_size = 3, stride=3)\n",
    "                                        )\n",
    "        \n",
    "        # auxiliary output layers\n",
    "        self.aux_lin_1= nn.Linear(32*3*3, 128)\n",
    "        self.aux_fc_1 = nn.Linear(128, 10)\n",
    "        self.aux_lin_2 = nn.Linear(32*3*3, 128)\n",
    "        self.aux_fc_2 = nn.Linear(128, 10)\n",
    "        \n",
    "        # summation label head\n",
    "        self.linear1 = nn.Linear(128*2, 128)\n",
    "        self.fc = nn.Linear(128, 19)\n",
    "        \n",
    "    def forward(self, x1, x2):\n",
    "        \n",
    "        # first input branch\n",
    "        x1 = F.relu(self.conv1(x1))\n",
    "        x1 = F.relu(self.conv2(x1))\n",
    "        x1 = F.relu(self.conv3(x1))\n",
    "        x1 = self.pooling(x1)\n",
    "        \n",
    "        # second input branch\n",
    "        x2 = self.input_branch_2(x2)\n",
    "        \n",
    "        # auxiliary outputs\n",
    "        x1 = x1.view(x1.shape[0], -1)\n",
    "        x1 = F.relu(self.aux_lin_1(x1))\n",
    "        x1_aux_output = self.aux_fc_1(x1)\n",
    "        \n",
    "        x2 = x2.view(x2.shape[0], -1)\n",
    "        x2 = F.relu(self.aux_lin_2(x2))\n",
    "        x2_aux_output = self.aux_fc_2(x2)\n",
    "        \n",
    "        # summation head\n",
    "        x = torch.cat((x1, x2), dim=1)\n",
    "        x = F.relu(self.linear1(x))\n",
    "        x = self.fc(x)\n",
    "        \n",
    "        return x1_aux_output, x2_aux_output, x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# build model\n",
    "net = SummationNet()\n",
    "\n",
    "print(\"Model structure: \", net)\n",
    "\n",
    "params = filter(lambda p: p.requires_grad, net.parameters())\n",
    "num_params = sum([np.prod(p.size()) for p in params])\n",
    "print(\"Model parameters: \", num_params)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# build optimizer\n",
    "optimizer = optim.SGD(net.parameters(), \n",
    "                      lr=LEARNING_RATE, \n",
    "                      momentum=0.9)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# build loss function\n",
    "cross_entropy_loss = nn.CrossEntropyLoss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# keeping track of metrics\n",
    "\n",
    "best_val_acc = 0.0\n",
    "training_losses = []\n",
    "training_accs = []\n",
    "val_losses = []\n",
    "val_accs = []\n",
    "\n",
    "training_aux1_accs = []\n",
    "training_aux2_accs = []\n",
    "val_aux1_accs = []\n",
    "val_aux2_accs = []\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# define a training epoch\n",
    "def do_training_epoch(model, dataloader, loss_func, optimizer, alpha=0.2):\n",
    "    \n",
    "    # init some metrics\n",
    "    num_instances = 0\n",
    "    num_correct = 0\n",
    "    running_loss = 0.0\n",
    "    \n",
    "    aux1_num_correct = 0\n",
    "    aux2_num_correct = 0\n",
    "    \n",
    "    \n",
    "    # set the model to be in \"train\" model\n",
    "    model.train()\n",
    "    \n",
    "    # iterate through the dataloader, batch by batch\n",
    "    \n",
    "    for i, batch in enumerate(dataloader):\n",
    "        \n",
    "        digit1, label1, digit2, label2, summation_labels = batch\n",
    "        \n",
    "        # pass to GPU?\n",
    "        \n",
    "        \n",
    "        # zero the gradients\n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        # make sure we are tracking gradients from here on out\n",
    "        with torch.set_grad_enabled(True):\n",
    "            \n",
    "            # pass data through the network\n",
    "            aux1, aux2, output = model(digit1, digit2)\n",
    "            \n",
    "            # compute the losses\n",
    "            aux1_loss = loss_func(aux1, label1)\n",
    "            aux2_loss = loss_func(aux2, label2)\n",
    "            summation_loss = loss_func(output, summation_labels)\n",
    "            \n",
    "            loss = summation_loss + alpha*aux1_loss + alpha*aux2_loss\n",
    "            \n",
    "            # call back-prop\n",
    "            loss.backward()\n",
    "            \n",
    "            # do a step of gradient descent\n",
    "            optimizer.step()\n",
    "        \n",
    "        \n",
    "        # now let's update our metrics\n",
    "        with torch.no_grad():\n",
    "            _, summation_preds = torch.max(output, 1)\n",
    "            running_loss += loss.item() * x.size(0)\n",
    "            \n",
    "            # summation acc metrics\n",
    "            num_correct += torch.sum(summation_preds == summation_labels.data)\n",
    "            num_instances += x.size(0)\n",
    "            \n",
    "            # aux heads\n",
    "            _, aux1_preds = torch.max(aux1, 1)\n",
    "            aux1_num_correct += torch.sum(aux1_preds == label1.data)\n",
    "            \n",
    "            _, aux2_preds = torch.max(aux2, 1)\n",
    "            aux2_num_correct += torch.sum(aux2_preds == label2.data)\n",
    "            \n",
    "            \n",
    "            \n",
    "    mean_loss = running_loss / num_instances\n",
    "    mean_accuracy = num_correct / num_instances\n",
    "    \n",
    "    mean_aux1_acc = aux1_num_correct / num_instances\n",
    "    mean_aux2_acc = aux2_num_correct / num_instances\n",
    "    return mean_accuracy, mean_loss, mean_aux1_acc, mean_aux2_acc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# define a validation epoch\n",
    "def do_validation_epoch(model, dataloader, loss_func, alpha=0.2):\n",
    "    # init some metrics\n",
    "    num_instances = 0\n",
    "    num_correct = 0\n",
    "    running_loss = 0.0\n",
    "    \n",
    "    aux1_num_correct = 0\n",
    "    aux2_num_correct = 0\n",
    "    \n",
    "    \n",
    "    # set the model to be in \"evaluation\" model\n",
    "    model.eval()\n",
    "    \n",
    "    # iterate through the dataloader, batch by batch\n",
    "    \n",
    "    for i, batch in enumerate(dataloader):\n",
    "        \n",
    "        digit1, label1, digit2, label2, summation_labels = batch\n",
    "        \n",
    "        # pass to GPU?\n",
    "        \n",
    "\n",
    "        # make sure we are tracking gradients from here on out\n",
    "        with torch.no_grad():\n",
    "            # pass data through the network\n",
    "            aux1, aux2, output = model(digit1, digit2)\n",
    "            \n",
    "            # compute the losses\n",
    "            aux1_loss = loss_func(aux1, label1)\n",
    "            aux2_loss = loss_func(aux2, label2)\n",
    "            summation_loss = loss_func(output, summation_labels)\n",
    "            \n",
    "            loss = summation_loss + alpha*aux1_loss + alpha*aux2_loss\n",
    "\n",
    "            # now let's update our metrics\n",
    "            _, summation_preds = torch.max(output, 1)\n",
    "            running_loss += loss.item() * x.size(0)\n",
    "            \n",
    "            # summation acc metrics\n",
    "            num_correct += torch.sum(summation_preds == summation_labels.data)\n",
    "            num_instances += x.size(0)\n",
    "            \n",
    "            # aux heads\n",
    "            _, aux1_preds = torch.max(aux1, 1)\n",
    "            aux1_num_correct += torch.sum(aux1_preds == label1.data)\n",
    "            \n",
    "            _, aux2_preds = torch.max(aux2, 1)\n",
    "            aux2_num_correct += torch.sum(aux2_preds == label2.data)\n",
    "            \n",
    "            \n",
    "    mean_loss = running_loss / num_instances\n",
    "    mean_accuracy = num_correct / num_instances\n",
    "    \n",
    "    mean_aux1_acc = aux1_num_correct / num_instances\n",
    "    mean_aux2_acc = aux2_num_correct / num_instances\n",
    "    \n",
    "    return mean_accuracy, mean_loss, mean_aux1_acc, mean_aux2_acc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# train model, \n",
    "for epoch in range(NUM_EPOCHS):\n",
    "    print(\"Let's do it up, epoch number: \", (epoch+1), \" of: \", NUM_EPOCHS)\n",
    "    \n",
    "    # train epoch\n",
    "    packed_ = do_training_epoch(net, train_dataloader, cross_entropy_loss, optimizer)\n",
    "    epoch_acc, epoch_loss, epoch_aux1_acc, epoch_aux2_acc = packed_\n",
    "    \n",
    "    print(\"Training loss: \", epoch_loss)\n",
    "    print(\"Summation accuracy: \", epoch_acc.item())\n",
    "    print(\"Aux1 and aux2 head accs: \", epoch_aux1_acc, epoch_aux2_acc)\n",
    "    \n",
    "    # update metrics\n",
    "    training_losses.append(epoch_loss)\n",
    "    training_accs.append(epoch_acc.item())\n",
    "    training_aux1_accs.append(epoch_aux1_acc.item())\n",
    "    training_aux2_accs.append(epoch_aux2_acc.item())\n",
    "    \n",
    "    \n",
    "    # val epoch\n",
    "    packed_ = do_validation_epoch(net, val_dataloader, cross_entropy_loss)\n",
    "    epoch_acc, epoch_loss, epoch_aux1_acc, epoch_aux2_acc = packed_\n",
    "    \n",
    "    print(\"Summation accuracy: \", epoch_acc.item())\n",
    "    print(\"Validation loss: \", epoch_loss)\n",
    "    print(\"Aux1 and aux2 head accs: \", epoch_aux1_acc, epoch_aux2_acc)\n",
    "    \n",
    "    \n",
    "    # update metrics\n",
    "    val_losses.append(epoch_loss)\n",
    "    val_accs.append(epoch_acc.item())\n",
    "    \n",
    "    val_aux1_accs.append(epoch_aux1_acc.item())\n",
    "    val_aux2_accs.append(epoch_aux2_acc.item())\n",
    "    \n",
    "    # is this the best epoch yet? if so, let's save the model\n",
    "    if epoch_acc > best_val_acc:\n",
    "        best_val_acc = epoch_acc\n",
    "        state_dict = {'weights': net.state_dict(),\n",
    "                     'epoch': epoch,\n",
    "                      'val_acc': epoch_acc.item()\n",
    "                     }\n",
    "        torch.save(state_dict, MODEL_WEIGHT_SAVE_PATH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# evaluation plots\n",
    "\n",
    "plt.plot(training_accs, color='r', label='Training')\n",
    "plt.plot(val_accs, color='k', label=\"Validation\")\n",
    "plt.title(\"Accuracy plots\")\n",
    "plt.ylabel(\"Acc.\")\n",
    "plt.xlabel(\"Epoch\")\n",
    "plt.legend()\n",
    "plt.show()\n",
    "plt.close()\n",
    "\n",
    "plt.plot(training_losses, color='r', label='Training')\n",
    "plt.plot(val_losses, color='k', label=\"Validation\")\n",
    "plt.title(\"Loss curves\")\n",
    "plt.ylabel(\"CE Loss\")\n",
    "plt.xlabel(\"Epoch\")\n",
    "plt.legend()\n",
    "plt.show()\n",
    "plt.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
