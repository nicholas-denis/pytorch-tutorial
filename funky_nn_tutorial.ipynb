{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# mnist addition using conv nets!!! multiple inputs (2 streams), \n",
    "# and multiple outputs (losses to predict each of the digits, plus addition)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-11-04T14:18:22.000266Z",
     "iopub.status.busy": "2022-11-04T14:18:21.999933Z",
     "iopub.status.idle": "2022-11-04T14:18:23.156389Z",
     "shell.execute_reply": "2022-11-04T14:18:23.155368Z",
     "shell.execute_reply.started": "2022-11-04T14:18:22.000189Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn.functional as F\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torchvision\n",
    "from torchvision import datasets, models, transforms\n",
    "from torch.utils.data import Dataset\n",
    "import time\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-11-04T14:18:23.853529Z",
     "iopub.status.busy": "2022-11-04T14:18:23.853221Z",
     "iopub.status.idle": "2022-11-04T14:18:23.858249Z",
     "shell.execute_reply": "2022-11-04T14:18:23.857240Z",
     "shell.execute_reply.started": "2022-11-04T14:18:23.853495Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# transforms\n",
    "mnist_transforms = transforms.Compose([\n",
    "                                        transforms.ToTensor(),\n",
    "                                        transforms.Normalize([0.5], [0.5])\n",
    "                                        ])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-11-04T14:18:25.360917Z",
     "iopub.status.busy": "2022-11-04T14:18:25.360499Z",
     "iopub.status.idle": "2022-11-04T14:18:25.426254Z",
     "shell.execute_reply": "2022-11-04T14:18:25.425238Z",
     "shell.execute_reply.started": "2022-11-04T14:18:25.360869Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# load train/validation MNIST datasets\n",
    "mnist_train_data = datasets.MNIST('/home/jovyan/MNIST/', train=True, download=False, transform=mnist_transforms)\n",
    "mnist_val_data = datasets.MNIST('/home/jovyan/MNIST/', train=False, download=False, transform=mnist_transforms)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-11-04T14:18:25.966606Z",
     "iopub.status.busy": "2022-11-04T14:18:25.966321Z",
     "iopub.status.idle": "2022-11-04T14:18:26.127481Z",
     "shell.execute_reply": "2022-11-04T14:18:26.126711Z",
     "shell.execute_reply.started": "2022-11-04T14:18:25.966576Z"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "60000\n",
      "<class 'tuple'>\n",
      "5\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAPsAAAD4CAYAAAAq5pAIAAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjQuMywgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy/MnkTPAAAACXBIWXMAAAsTAAALEwEAmpwYAAAOX0lEQVR4nO3dbYxc5XnG8euKbUwxJvHGseMQFxzjFAg0Jl0ZkBFQoVCCIgGKCLGiiFBapwlOQutKUFoVWtHKrRIiSimSKS6m4iWQgPAHmsSyECRqcFmoAROHN+MS4+0aswIDIfZ6fffDjqsFdp5dZs68eO//T1rNzLnnzLk1cPmcmeeceRwRAjD5faDTDQBoD8IOJEHYgSQIO5AEYQeSmNrOjR3i6XGoZrRzk0Aqv9Fb2ht7PFatqbDbPkfS9ZKmSPrXiFhVev6hmqGTfVYzmwRQsDE21K01fBhve4qkGyV9TtLxkpbZPr7R1wPQWs18Zl8i6fmI2BoReyXdJem8atoCULVmwn6kpF+Nery9tuwdbC+33We7b0h7mtgcgGY0E/axvgR4z7m3EbE6InojoneapjexOQDNaCbs2yXNH/X445J2NNcOgFZpJuyPSlpke4HtQyR9SdK6atoCULWGh94iYp/tFZJ+rJGhtzUR8XRlnQGoVFPj7BHxgKQHKuoFQAtxuiyQBGEHkiDsQBKEHUiCsANJEHYgCcIOJEHYgSQIO5AEYQeSIOxAEoQdSIKwA0kQdiAJwg4kQdiBJAg7kARhB5Ig7EAShB1IgrADSRB2IAnCDiRB2IEkCDuQBGEHkiDsQBKEHUiCsANJNDWLK7qfp5b/E0/5yOyWbv+ZPz+6bm34sP3FdY9auLNYP+wbLtb/97pD6tYe7/1+cd1dw28V6yffs7JYP+bPHinWO6GpsNveJukNScOS9kVEbxVNAaheFXv234+IXRW8DoAW4jM7kESzYQ9JP7H9mO3lYz3B9nLbfbb7hrSnyc0BaFSzh/FLI2KH7TmS1tv+ZUQ8PPoJEbFa0mpJOsI90eT2ADSoqT17ROyo3e6UdJ+kJVU0BaB6DYfd9gzbMw/cl3S2pM1VNQagWs0cxs+VdJ/tA69zR0T8qJKuJpkpxy0q1mP6tGJ9xxkfKtbfPqX+mHDPB8vjxT/9dHm8uZP+49czi/V/+OdzivWNJ95Rt/bi0NvFdVcNfLZY/9hPD75PpA2HPSK2Svp0hb0AaCGG3oAkCDuQBGEHkiDsQBKEHUiCS1wrMHzmZ4r16269sVj/5LT6l2JOZkMxXKz/9Q1fLdanvlUe/jr1nhV1azNf3ldcd/qu8tDcYX0bi/VuxJ4dSIKwA0kQdiAJwg4kQdiBJAg7kARhB5JgnL0C05/ZUaw/9pv5xfonpw1U2U6lVvafUqxvfbP8U9S3LvxB3drr+8vj5HP/6T+L9VY6+C5gHR97diAJwg4kQdiBJAg7kARhB5Ig7EAShB1IwhHtG1E8wj1xss9q2/a6xeAlpxbru88p/9zzlCcPL9af+MYN77unA67d9bvF+qNnlMfRh197vViPU+v/APG2bxVX1YJlT5SfgPfYGBu0OwbHnMuaPTuQBGEHkiDsQBKEHUiCsANJEHYgCcIOJME4exeYMvvDxfrwq4PF+ot31B8rf/r0NcV1l/z9N4v1OTd27ppyvH9NjbPbXmN7p+3No5b12F5v+7na7awqGwZQvYkcxt8q6d2z3l8paUNELJK0ofYYQBcbN+wR8bCkdx9Hnidpbe3+WknnV9sWgKo1+gXd3Ijol6Ta7Zx6T7S93Haf7b4h7WlwcwCa1fJv4yNidUT0RkTvNE1v9eYA1NFo2Adsz5Ok2u3O6loC0AqNhn2dpItr9y+WdH817QBolXF/N972nZLOlDTb9nZJV0taJelu25dKeknSha1scrIb3vVqU+sP7W58fvdPffkXxforN00pv8D+8hzr6B7jhj0iltUpcXYMcBDhdFkgCcIOJEHYgSQIO5AEYQeSYMrmSeC4K56tW7vkxPKgyb8dtaFYP+PCy4r1md9/pFhH92DPDiRB2IEkCDuQBGEHkiDsQBKEHUiCsANJMM4+CZSmTX7168cV131p3dvF+pXX3las/8UXLyjW478/WLc2/+9+XlxXbfyZ8wzYswNJEHYgCcIOJEHYgSQIO5AEYQeSIOxAEkzZnNzgH55arN9+9XeK9QVTD21425+6bUWxvujm/mJ939ZtDW97smpqymYAkwNhB5Ig7EAShB1IgrADSRB2IAnCDiTBODuKYuniYv2IVduL9Ts/8eOGt33sg39UrP/O39S/jl+Shp/b2vC2D1ZNjbPbXmN7p+3No5ZdY/tl25tqf+dW2TCA6k3kMP5WSeeMsfx7EbG49vdAtW0BqNq4YY+IhyUNtqEXAC3UzBd0K2w/WTvMn1XvSbaX2+6z3TekPU1sDkAzGg37TZIWSlosqV/Sd+s9MSJWR0RvRPRO0/QGNwegWQ2FPSIGImI4IvZLulnSkmrbAlC1hsJue96ohxdI2lzvuQC6w7jj7LbvlHSmpNmSBiRdXXu8WFJI2ibpaxFRvvhYjLNPRlPmzinWd1x0TN3axiuuL677gXH2RV9+8exi/fXTXi3WJ6PSOPu4k0RExLIxFt/SdFcA2orTZYEkCDuQBGEHkiDsQBKEHUiCS1zRMXdvL0/ZfJgPKdZ/HXuL9c9/8/L6r33fxuK6Byt+ShoAYQeyIOxAEoQdSIKwA0kQdiAJwg4kMe5Vb8ht/2mLi/UXLixP2XzC4m11a+ONo4/nhsGTivXD7u9r6vUnG/bsQBKEHUiCsANJEHYgCcIOJEHYgSQIO5AE4+yTnHtPKNaf/VZ5rPvmpWuL9dMPLV9T3ow9MVSsPzK4oPwC+8f9dfNU2LMDSRB2IAnCDiRB2IEkCDuQBGEHkiDsQBKMsx8Epi44qlh/4ZKP1a1dc9FdxXW/cPiuhnqqwlUDvcX6Q9efUqzPWlv+3Xm807h7dtvzbT9oe4vtp21/u7a8x/Z628/Vbme1vl0AjZrIYfw+SSsj4jhJp0i6zPbxkq6UtCEiFknaUHsMoEuNG/aI6I+Ix2v335C0RdKRks6TdOBcyrWSzm9RjwAq8L6+oLN9tKSTJG2UNDci+qWRfxAkzamzznLbfbb7hrSnyXYBNGrCYbd9uKQfSro8InZPdL2IWB0RvRHRO03TG+kRQAUmFHbb0zQS9Nsj4t7a4gHb82r1eZJ2tqZFAFUYd+jNtiXdImlLRFw3qrRO0sWSVtVu729Jh5PA1KN/u1h//ffmFesX/e2PivU/+dC9xXorrewvD4/9/F/qD6/13PpfxXVn7WdorUoTGWdfKukrkp6yvam27CqNhPxu25dKeknShS3pEEAlxg17RPxM0piTu0s6q9p2ALQKp8sCSRB2IAnCDiRB2IEkCDuQBJe4TtDUeR+tWxtcM6O47tcXPFSsL5s50FBPVVjx8mnF+uM3LS7WZ/9gc7He8wZj5d2CPTuQBGEHkiDsQBKEHUiCsANJEHYgCcIOJJFmnH3vH5R/tnjvnw4W61cd80Dd2tm/9VZDPVVlYPjturXT160srnvsX/2yWO95rTxOvr9YRTdhzw4kQdiBJAg7kARhB5Ig7EAShB1IgrADSaQZZ992fvnftWdPvKdl277xtYXF+vUPnV2se7jej/uOOPbaF+vWFg1sLK47XKxiMmHPDiRB2IEkCDuQBGEHkiDsQBKEHUiCsANJOCLKT7DnS7pN0kc1cvny6oi43vY1kv5Y0iu1p14VEfUv+pZ0hHviZDPxK9AqG2ODdsfgmCdmTOSkmn2SVkbE47ZnSnrM9vpa7XsR8Z2qGgXQOhOZn71fUn/t/hu2t0g6stWNAajW+/rMbvtoSSdJOnAO5grbT9peY3tWnXWW2+6z3TekPc11C6BhEw677cMl/VDS5RGxW9JNkhZKWqyRPf93x1ovIlZHRG9E9E7T9OY7BtCQCYXd9jSNBP32iLhXkiJiICKGI2K/pJslLWldmwCaNW7YbVvSLZK2RMR1o5bPG/W0CySVp/ME0FET+TZ+qaSvSHrK9qbasqskLbO9WFJI2ibpay3oD0BFJvJt/M8kjTVuVxxTB9BdOIMOSIKwA0kQdiAJwg4kQdiBJAg7kARhB5Ig7EAShB1IgrADSRB2IAnCDiRB2IEkCDuQxLg/JV3pxuxXJP3PqEWzJe1qWwPvT7f21q19SfTWqCp7OyoiPjJWoa1hf8/G7b6I6O1YAwXd2lu39iXRW6Pa1RuH8UAShB1IotNhX93h7Zd0a2/d2pdEb41qS28d/cwOoH06vWcH0CaEHUiiI2G3fY7tZ2w/b/vKTvRQj+1ttp+yvcl2X4d7WWN7p+3No5b12F5v+7na7Zhz7HWot2tsv1x77zbZPrdDvc23/aDtLbaftv3t2vKOvneFvtryvrX9M7vtKZKelfRZSdslPSppWUT8oq2N1GF7m6TeiOj4CRi2T5f0pqTbIuKE2rJ/lDQYEatq/1DOiogruqS3ayS92elpvGuzFc0bPc24pPMlfVUdfO8KfX1RbXjfOrFnXyLp+YjYGhF7Jd0l6bwO9NH1IuJhSYPvWnyepLW1+2s18j9L29XprStERH9EPF67/4akA9OMd/S9K/TVFp0I+5GSfjXq8XZ113zvIeknth+zvbzTzYxhbkT0SyP/80ia0+F+3m3cabzb6V3TjHfNe9fI9OfN6kTYx5pKqpvG/5ZGxGckfU7SZbXDVUzMhKbxbpcxphnvCo1Of96sToR9u6T5ox5/XNKODvQxpojYUbvdKek+dd9U1AMHZtCt3e7scD//r5um8R5rmnF1wXvXyenPOxH2RyUtsr3A9iGSviRpXQf6eA/bM2pfnMj2DElnq/umol4n6eLa/Ysl3d/BXt6hW6bxrjfNuDr83nV8+vOIaPufpHM18o38C5L+shM91OnrE5KeqP093eneJN2pkcO6IY0cEV0q6cOSNkh6rnbb00W9/bukpyQ9qZFgzetQb6dp5KPhk5I21f7O7fR7V+irLe8bp8sCSXAGHZAEYQeSIOxAEoQdSIKwA0kQdiAJwg4k8X+zhHFo7nUhhwAAAABJRU5ErkJggg==\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "print(len(mnist_train_data))\n",
    "print(type(mnist_train_data[0]))\n",
    "img, label = mnist_train_data[0]\n",
    "print(label)\n",
    "plt.imshow(img.reshape(28,28))\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-11-04T14:18:27.121551Z",
     "iopub.status.busy": "2022-11-04T14:18:27.121191Z",
     "iopub.status.idle": "2022-11-04T14:18:27.127815Z",
     "shell.execute_reply": "2022-11-04T14:18:27.127049Z",
     "shell.execute_reply.started": "2022-11-04T14:18:27.121513Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "class Mnist_Custom_Dataset(Dataset):\n",
    "    \"\"\"\n",
    "    Custom MNIST dataset that returns 2 digits (images), their respective labels\n",
    "    and the summation of the two numbers (labels).\n",
    "    \"\"\"\n",
    "    def __init__(self, images, labels):\n",
    "        \"\"\"\n",
    "        args:\n",
    "        \n",
    "        images (torch.tensor): tensor of all the inputs (images) for a dataset\n",
    "        labels (torch.tensor): tensor of the labels corresponding to inputs\n",
    "        \"\"\"\n",
    "        \n",
    "        self.images = images\n",
    "        self.labels = labels\n",
    "        self.N = len(self.labels)\n",
    "\n",
    "    def __len__(self):\n",
    "        return self.N\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        # dataset will return 2 digits and their labels, as well as their summation as the \"true\" label\n",
    "        first_image = self.images[idx]\n",
    "        first_label = self.labels[idx]\n",
    "        \n",
    "        # sample a random image for the 2nd digit\n",
    "        second_idx = torch.randint(0, self.N, (1,)).item()\n",
    "        second_image = self.images[second_idx]\n",
    "        second_label = self.labels[second_idx]\n",
    "        \n",
    "        summation_label = first_label + second_label\n",
    "        \n",
    "        return first_image, first_label, second_image, second_label, summation_label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-11-04T14:18:27.387600Z",
     "iopub.status.busy": "2022-11-04T14:18:27.387289Z",
     "iopub.status.idle": "2022-11-04T14:18:47.782260Z",
     "shell.execute_reply": "2022-11-04T14:18:47.781450Z",
     "shell.execute_reply.started": "2022-11-04T14:18:27.387566Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# separate the inputs from labels\n",
    "mnist_train_inputs = [mnist_train_data[i][0] for i in range(len(mnist_train_data))]\n",
    "mnist_train_labels = [torch.tensor(mnist_train_data[i][1]) for i in range(len(mnist_train_data))]\n",
    "\n",
    "mnist_val_inputs = [mnist_val_data[i][0] for i in range(len(mnist_val_data))]\n",
    "mnist_val_labels = [torch.tensor(mnist_val_data[i][1]) for i in range(len(mnist_val_data))]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-11-04T14:18:47.783777Z",
     "iopub.status.busy": "2022-11-04T14:18:47.783578Z",
     "iopub.status.idle": "2022-11-04T14:18:48.188789Z",
     "shell.execute_reply": "2022-11-04T14:18:48.188057Z",
     "shell.execute_reply.started": "2022-11-04T14:18:47.783753Z"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([60000, 1, 28, 28])\n",
      "torch.Size([60000])\n",
      "torch.Size([10000, 1, 28, 28])\n",
      "torch.Size([10000])\n"
     ]
    }
   ],
   "source": [
    "mnist_train_inputs = torch.stack(mnist_train_inputs)\n",
    "print(mnist_train_inputs.shape)\n",
    "mnist_train_labels = torch.stack(mnist_train_labels)\n",
    "print(mnist_train_labels.shape)\n",
    "\n",
    "mnist_val_inputs = torch.stack(mnist_val_inputs)\n",
    "print(mnist_val_inputs.shape)\n",
    "mnist_val_labels = torch.stack(mnist_val_labels)\n",
    "print(mnist_val_labels.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-11-04T14:18:48.190477Z",
     "iopub.status.busy": "2022-11-04T14:18:48.190282Z",
     "iopub.status.idle": "2022-11-04T14:18:48.193709Z",
     "shell.execute_reply": "2022-11-04T14:18:48.193061Z",
     "shell.execute_reply.started": "2022-11-04T14:18:48.190452Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# now build our custom dataset\n",
    "custom_mnist_train_dataset = Mnist_Custom_Dataset(mnist_train_inputs, mnist_train_labels)\n",
    "custom_mnist_val_dataset = Mnist_Custom_Dataset(mnist_val_inputs, mnist_val_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-11-04T14:18:48.195063Z",
     "iopub.status.busy": "2022-11-04T14:18:48.194875Z",
     "iopub.status.idle": "2022-11-04T14:18:48.213125Z",
     "shell.execute_reply": "2022-11-04T14:18:48.212287Z",
     "shell.execute_reply.started": "2022-11-04T14:18:48.195040Z"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(5) tensor(0) tensor(5)\n"
     ]
    }
   ],
   "source": [
    "# just get data from the dataset\n",
    "digit_1, label_1, digit_2, label_2, summation_label = custom_mnist_train_dataset.__getitem__(0)\n",
    "print(label_1, label_2, summation_label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-11-04T14:18:48.214364Z",
     "iopub.status.busy": "2022-11-04T14:18:48.214168Z",
     "iopub.status.idle": "2022-11-04T14:18:48.229131Z",
     "shell.execute_reply": "2022-11-04T14:18:48.228219Z",
     "shell.execute_reply.started": "2022-11-04T14:18:48.214340Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# define some hypers\n",
    "BATCH_SIZE = 32\n",
    "LEARNING_RATE = 0.01\n",
    "NUM_EPOCHS = 5\n",
    "MODEL_WEIGHT_SAVE_PATH = '/home/jovyan/best-mnist-summation-val-weights.pth'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-11-04T14:18:48.230290Z",
     "iopub.status.busy": "2022-11-04T14:18:48.230107Z",
     "iopub.status.idle": "2022-11-04T14:18:48.254715Z",
     "shell.execute_reply": "2022-11-04T14:18:48.253936Z",
     "shell.execute_reply.started": "2022-11-04T14:18:48.230268Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# data loader\n",
    "train_dataloader = torch.utils.data.DataLoader(custom_mnist_train_dataset,\n",
    "                                              batch_size = BATCH_SIZE,\n",
    "                                              shuffle = True\n",
    "                                              )\n",
    "\n",
    "val_dataloader = torch.utils.data.DataLoader(custom_mnist_val_dataset,\n",
    "                                              batch_size = BATCH_SIZE,\n",
    "                                              shuffle = False\n",
    "                                              )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-11-04T14:18:48.256118Z",
     "iopub.status.busy": "2022-11-04T14:18:48.255886Z",
     "iopub.status.idle": "2022-11-04T14:18:48.277436Z",
     "shell.execute_reply": "2022-11-04T14:18:48.276724Z",
     "shell.execute_reply.started": "2022-11-04T14:18:48.256083Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# define a neural network\n",
    "class SummationNet(nn.Module):\n",
    "    \"\"\"\n",
    "    A CNN that has 2 input channels, and 3 output heads.\n",
    "    This CNN takes 2 MNIST images and inputs and aims to \n",
    "    do summation of the 2 digits (integers).\n",
    "    \n",
    "    \"\"\"\n",
    "    def __init__(self):\n",
    "        super(SummationNet, self).__init__()\n",
    "        \n",
    "        # NOTE: input shape is (1, 28, 28) \n",
    "        \n",
    "        # should we use 2 separate input branches or use \"parameter\" sharing\n",
    "        # via a single input branch?\n",
    "        self.conv1 = nn.Conv2d(1, 16, 3, stride=2)\n",
    "        self.conv2 = nn.Conv2d(16, 16, 3)\n",
    "        self.conv3 = nn.Conv2d(16, 32, 3)\n",
    "        self.pooling = nn.AvgPool2d(kernel_size = 3, stride=3)\n",
    "        # representation will now be of shape (batch_size, 32, 3, 3)\n",
    "\n",
    "        # sequential\n",
    "        self.input_branch_2 = nn.Sequential(\n",
    "                                          nn.Conv2d(1, 16, 3, stride=2),\n",
    "                                          nn.ReLU(),\n",
    "                                          nn.Conv2d(16, 16, 3),\n",
    "                                          nn.ReLU(),\n",
    "                                          nn.Conv2d(16, 32, 3),\n",
    "                                          nn.ReLU(),\n",
    "                                          nn.AvgPool2d(kernel_size = 3, stride=3)\n",
    "                                        )\n",
    "        \n",
    "        # auxiliary output layers\n",
    "        self.aux_lin_1= nn.Linear(32*3*3, 128)\n",
    "        self.aux_fc_1 = nn.Linear(128, 10) # classification for input 1 happens\n",
    "        self.aux_lin_2 = nn.Linear(32*3*3, 128)\n",
    "        self.aux_fc_2 = nn.Linear(128, 10) # classification for input 2 happens\n",
    "        \n",
    "        # summation label head\n",
    "        self.linear1 = nn.Linear(128*2, 128)\n",
    "        self.fc = nn.Linear(128, 19)\n",
    "        \n",
    "    def forward(self, x1, x2):\n",
    "        \n",
    "        # first input branch\n",
    "        x1 = F.relu(self.conv1(x1))\n",
    "        x1 = F.relu(self.conv2(x1))\n",
    "        x1 = F.relu(self.conv3(x1))\n",
    "        x1 = self.pooling(x1)\n",
    "        \n",
    "        # second input branch\n",
    "        x2 = self.input_branch_2(x2)\n",
    "        \n",
    "        # auxiliary outputs\n",
    "        x1 = x1.view(x1.shape[0], -1)\n",
    "        x1 = F.relu(self.aux_lin_1(x1))\n",
    "        x1_aux_output = self.aux_fc_1(x1)\n",
    "        \n",
    "        x2 = x2.view(x2.shape[0], -1)\n",
    "        x2 = F.relu(self.aux_lin_2(x2))\n",
    "        x2_aux_output = self.aux_fc_2(x2)\n",
    "        \n",
    "        # summation head\n",
    "        x = torch.cat((x1, x2), dim=1)\n",
    "        x = F.relu(self.linear1(x))\n",
    "        x = self.fc(x)\n",
    "        \n",
    "        return x1_aux_output, x2_aux_output, x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-11-04T14:18:48.280007Z",
     "iopub.status.busy": "2022-11-04T14:18:48.279723Z",
     "iopub.status.idle": "2022-11-04T14:18:48.306884Z",
     "shell.execute_reply": "2022-11-04T14:18:48.305953Z",
     "shell.execute_reply.started": "2022-11-04T14:18:48.279969Z"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model structure:  SummationNet(\n",
      "  (conv1): Conv2d(1, 16, kernel_size=(3, 3), stride=(2, 2))\n",
      "  (conv2): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1))\n",
      "  (conv3): Conv2d(16, 32, kernel_size=(3, 3), stride=(1, 1))\n",
      "  (pooling): AvgPool2d(kernel_size=3, stride=3, padding=0)\n",
      "  (input_branch_2): Sequential(\n",
      "    (0): Conv2d(1, 16, kernel_size=(3, 3), stride=(2, 2))\n",
      "    (1): ReLU()\n",
      "    (2): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1))\n",
      "    (3): ReLU()\n",
      "    (4): Conv2d(16, 32, kernel_size=(3, 3), stride=(1, 1))\n",
      "    (5): ReLU()\n",
      "    (6): AvgPool2d(kernel_size=3, stride=3, padding=0)\n",
      "  )\n",
      "  (aux_lin_1): Linear(in_features=288, out_features=128, bias=True)\n",
      "  (aux_fc_1): Linear(in_features=128, out_features=10, bias=True)\n",
      "  (aux_lin_2): Linear(in_features=288, out_features=128, bias=True)\n",
      "  (aux_fc_2): Linear(in_features=128, out_features=10, bias=True)\n",
      "  (linear1): Linear(in_features=256, out_features=128, bias=True)\n",
      "  (fc): Linear(in_features=128, out_features=19, bias=True)\n",
      ")\n",
      "Model parameters:  126151\n"
     ]
    }
   ],
   "source": [
    "# build model\n",
    "net = SummationNet()\n",
    "\n",
    "print(\"Model structure: \", net)\n",
    "\n",
    "params = filter(lambda p: p.requires_grad, net.parameters())\n",
    "num_params = sum([np.prod(p.size()) for p in params])\n",
    "print(\"Model parameters: \", num_params)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-11-04T14:18:50.867931Z",
     "iopub.status.busy": "2022-11-04T14:18:50.867654Z",
     "iopub.status.idle": "2022-11-04T14:18:50.871944Z",
     "shell.execute_reply": "2022-11-04T14:18:50.871162Z",
     "shell.execute_reply.started": "2022-11-04T14:18:50.867901Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# build optimizer\n",
    "optimizer = optim.SGD(net.parameters(), \n",
    "                      lr=LEARNING_RATE, \n",
    "                      momentum=0.9)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-11-04T14:18:52.215586Z",
     "iopub.status.busy": "2022-11-04T14:18:52.215261Z",
     "iopub.status.idle": "2022-11-04T14:18:52.219508Z",
     "shell.execute_reply": "2022-11-04T14:18:52.218516Z",
     "shell.execute_reply.started": "2022-11-04T14:18:52.215553Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# build loss function\n",
    "cross_entropy_loss = nn.CrossEntropyLoss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-11-04T14:18:54.114719Z",
     "iopub.status.busy": "2022-11-04T14:18:54.114418Z",
     "iopub.status.idle": "2022-11-04T14:18:54.119064Z",
     "shell.execute_reply": "2022-11-04T14:18:54.118242Z",
     "shell.execute_reply.started": "2022-11-04T14:18:54.114686Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# keeping track of metrics\n",
    "\n",
    "best_val_acc = 0.0\n",
    "training_losses = []\n",
    "training_accs = []\n",
    "val_losses = []\n",
    "val_accs = []\n",
    "\n",
    "training_aux1_accs = []\n",
    "training_aux2_accs = []\n",
    "val_aux1_accs = []\n",
    "val_aux2_accs = []\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-11-04T14:19:19.431691Z",
     "iopub.status.busy": "2022-11-04T14:19:19.431371Z",
     "iopub.status.idle": "2022-11-04T14:19:19.507004Z",
     "shell.execute_reply": "2022-11-04T14:19:19.506150Z",
     "shell.execute_reply.started": "2022-11-04T14:19:19.431657Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# define a training epoch\n",
    "def do_training_epoch(model, dataloader, loss_func, optimizer, alpha=0.2):\n",
    "    \n",
    "    # init some metrics\n",
    "    num_instances = 0\n",
    "    num_correct = 0\n",
    "    running_loss = 0.0\n",
    "    \n",
    "    aux1_num_correct = 0\n",
    "    aux2_num_correct = 0\n",
    "    \n",
    "    \n",
    "    # set the model to be in \"train\" model\n",
    "    model.train()\n",
    "    \n",
    "    # iterate through the dataloader, batch by batch\n",
    "    \n",
    "    for i, batch in enumerate(dataloader):\n",
    "        \n",
    "        digit1, label1, digit2, label2, summation_labels = batch\n",
    "        \n",
    "        # pass to GPU?\n",
    "        \n",
    "        \n",
    "        # zero the gradients\n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        # make sure we are tracking gradients from here on out\n",
    "        with torch.set_grad_enabled(True):\n",
    "            \n",
    "            # pass data through the network\n",
    "            aux1, aux2, output = model(digit1, digit2)\n",
    "            \n",
    "            # compute the losses\n",
    "            aux1_loss = loss_func(aux1, label1)\n",
    "            aux2_loss = loss_func(aux2, label2)\n",
    "            summation_loss = loss_func(output, summation_labels)\n",
    "            \n",
    "            loss = summation_loss + alpha*aux1_loss + alpha*aux2_loss\n",
    "            \n",
    "            # call back-prop\n",
    "            loss.backward()\n",
    "            \n",
    "            # do a step of gradient descent\n",
    "            optimizer.step()\n",
    "        \n",
    "        \n",
    "        # now let's update our metrics\n",
    "        with torch.no_grad():\n",
    "            _, summation_preds = torch.max(output, 1)\n",
    "            running_loss += loss.item() * digit1.size(0)\n",
    "            \n",
    "            # summation acc metrics\n",
    "            num_correct += torch.sum(summation_preds == summation_labels.data)\n",
    "            num_instances += digit1.size(0)\n",
    "            \n",
    "            # aux heads\n",
    "            _, aux1_preds = torch.max(aux1, 1)\n",
    "            aux1_num_correct += torch.sum(aux1_preds == label1.data)\n",
    "            \n",
    "            _, aux2_preds = torch.max(aux2, 1)\n",
    "            aux2_num_correct += torch.sum(aux2_preds == label2.data)\n",
    "            \n",
    "            \n",
    "            \n",
    "    mean_loss = running_loss / num_instances\n",
    "    mean_accuracy = num_correct / num_instances\n",
    "    \n",
    "    mean_aux1_acc = aux1_num_correct / num_instances\n",
    "    mean_aux2_acc = aux2_num_correct / num_instances\n",
    "    return mean_accuracy, mean_loss, mean_aux1_acc, mean_aux2_acc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-11-04T14:22:09.789060Z",
     "iopub.status.busy": "2022-11-04T14:22:09.788718Z",
     "iopub.status.idle": "2022-11-04T14:22:09.796773Z",
     "shell.execute_reply": "2022-11-04T14:22:09.795944Z",
     "shell.execute_reply.started": "2022-11-04T14:22:09.789023Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# define a validation epoch\n",
    "def do_validation_epoch(model, dataloader, loss_func, alpha=0.2):\n",
    "    # init some metrics\n",
    "    num_instances = 0\n",
    "    num_correct = 0\n",
    "    running_loss = 0.0\n",
    "    \n",
    "    aux1_num_correct = 0\n",
    "    aux2_num_correct = 0\n",
    "    \n",
    "    \n",
    "    # set the model to be in \"evaluation\" model\n",
    "    model.eval()\n",
    "    \n",
    "    # iterate through the dataloader, batch by batch\n",
    "    \n",
    "    for i, batch in enumerate(dataloader):\n",
    "        \n",
    "        digit1, label1, digit2, label2, summation_labels = batch\n",
    "        \n",
    "        # pass to GPU?\n",
    "        \n",
    "\n",
    "        # make sure we are tracking gradients from here on out\n",
    "        with torch.no_grad():\n",
    "            # pass data through the network\n",
    "            aux1, aux2, output = model(digit1, digit2)\n",
    "            \n",
    "            # compute the losses\n",
    "            aux1_loss = loss_func(aux1, label1)\n",
    "            aux2_loss = loss_func(aux2, label2)\n",
    "            summation_loss = loss_func(output, summation_labels)\n",
    "            \n",
    "            loss = summation_loss + alpha*aux1_loss + alpha*aux2_loss\n",
    "\n",
    "            # now let's update our metrics\n",
    "            _, summation_preds = torch.max(output, 1)\n",
    "            running_loss += loss.item() * label1.size(0)\n",
    "            \n",
    "            # summation acc metrics\n",
    "            num_correct += torch.sum(summation_preds == summation_labels.data)\n",
    "            num_instances += summation_labels.size(0)\n",
    "            \n",
    "            # aux heads\n",
    "            _, aux1_preds = torch.max(aux1, 1)\n",
    "            aux1_num_correct += torch.sum(aux1_preds == label1.data)\n",
    "            \n",
    "            _, aux2_preds = torch.max(aux2, 1)\n",
    "            aux2_num_correct += torch.sum(aux2_preds == label2.data)\n",
    "            \n",
    "            \n",
    "    mean_loss = running_loss / num_instances\n",
    "    mean_accuracy = num_correct / num_instances\n",
    "    \n",
    "    mean_aux1_acc = aux1_num_correct / num_instances\n",
    "    mean_aux2_acc = aux2_num_correct / num_instances\n",
    "    \n",
    "    return mean_accuracy, mean_loss, mean_aux1_acc, mean_aux2_acc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-11-04T14:22:10.340592Z",
     "iopub.status.busy": "2022-11-04T14:22:10.340291Z"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Let's do it up, epoch number:  1  of:  5\n",
      "Training loss:  0.2994303427437941\n",
      "Training Summation accuracy:  0.9249500036239624\n",
      "Training Aux1 and aux2 head accs:  tensor(0.9695) tensor(0.9725)\n",
      "\n",
      "Validation loss:  0.20799413625895977\n",
      "Validation Summation accuracy:  0.9455999732017517\n",
      "Validation Aux1 and aux2 head accs:  tensor(0.9800) tensor(0.9808)\n",
      "\n",
      "Let's do it up, epoch number:  2  of:  5\n",
      "Training loss:  0.18769949758425356\n",
      "Training Summation accuracy:  0.9536499977111816\n",
      "Training Aux1 and aux2 head accs:  tensor(0.9811) tensor(0.9823)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# train model, \n",
    "for epoch in range(NUM_EPOCHS):\n",
    "    print(\"Let's do it up, epoch number: \", (epoch+1), \" of: \", NUM_EPOCHS)\n",
    "    \n",
    "    # train epoch\n",
    "    packed_ = do_training_epoch(net, train_dataloader, cross_entropy_loss, optimizer)\n",
    "    epoch_acc, epoch_loss, epoch_aux1_acc, epoch_aux2_acc = packed_\n",
    "    \n",
    "    print(\"Training loss: \", epoch_loss)\n",
    "    print(\"Training Summation accuracy: \", epoch_acc.item())\n",
    "    print(\"Training Aux1 and aux2 head accs: \", epoch_aux1_acc, epoch_aux2_acc)\n",
    "    print(\"\")\n",
    "    \n",
    "    # update metrics\n",
    "    training_losses.append(epoch_loss)\n",
    "    training_accs.append(epoch_acc.item())\n",
    "    training_aux1_accs.append(epoch_aux1_acc.item())\n",
    "    training_aux2_accs.append(epoch_aux2_acc.item())\n",
    "    \n",
    "    \n",
    "    # val epoch\n",
    "    packed_ = do_validation_epoch(net, val_dataloader, cross_entropy_loss)\n",
    "    epoch_acc, epoch_loss, epoch_aux1_acc, epoch_aux2_acc = packed_\n",
    "    \n",
    "    print(\"Validation loss: \", epoch_loss)\n",
    "    print(\"Validation Summation accuracy: \", epoch_acc.item())\n",
    "    print(\"Validation Aux1 and aux2 head accs: \", epoch_aux1_acc, epoch_aux2_acc)\n",
    "    print(\"\")\n",
    "    \n",
    "    # update metrics\n",
    "    val_losses.append(epoch_loss)\n",
    "    val_accs.append(epoch_acc.item())\n",
    "    \n",
    "    val_aux1_accs.append(epoch_aux1_acc.item())\n",
    "    val_aux2_accs.append(epoch_aux2_acc.item())\n",
    "    \n",
    "    # is this the best epoch yet? if so, let's save the model\n",
    "    if epoch_acc > best_val_acc:\n",
    "        best_val_acc = epoch_acc\n",
    "        state_dict = {'weights': net.state_dict(),\n",
    "                     'epoch': epoch,\n",
    "                      'val_acc': epoch_acc.item()\n",
    "                     }\n",
    "        torch.save(state_dict, MODEL_WEIGHT_SAVE_PATH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.status.busy": "2022-10-28T19:02:37.866309Z",
     "iopub.status.idle": "2022-10-28T19:02:37.866570Z",
     "shell.execute_reply": "2022-10-28T19:02:37.866443Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# evaluation plots\n",
    "\n",
    "plt.plot(training_accs, color='r', label='Training')\n",
    "plt.plot(val_accs, color='k', label=\"Validation\")\n",
    "plt.title(\"Accuracy plots\")\n",
    "plt.ylabel(\"Acc.\")\n",
    "plt.xlabel(\"Epoch\")\n",
    "plt.legend()\n",
    "plt.show()\n",
    "plt.close()\n",
    "\n",
    "plt.plot(training_losses, color='r', label='Training')\n",
    "plt.plot(val_losses, color='k', label=\"Validation\")\n",
    "plt.title(\"Loss curves\")\n",
    "plt.ylabel(\"CE Loss\")\n",
    "plt.xlabel(\"Epoch\")\n",
    "plt.legend()\n",
    "plt.show()\n",
    "plt.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
